---
title: "HW3"
author: "Elle Boodsakorn"
date: "4/6/2022"
output:
  md_document:
    variant: markdown_github
always_allow_html: yes
---
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
load.libraries <- c('tidyverse', 'mosaic', 'MASS', 'FNN', 'gamlr', 'glmnet', 'randomForest', 'kableExtra', 'data.table', 'sjmisc', 'sjPlot', 'sjlabelled', 'LICORS', 'foreach', 'cluster', 'factoextra', 'knitr', 'GGally', 'scales', 'lubridate', 'gridExtra', 'grid', 'lattice',  'arules', 'arulesViz', 'igraph', 'png')
install.lib <- load.libraries[!load.libraries %in% installed.packages()]
for(libs in install.lib) install.packages(libs, dependences = TRUE)
sapply(load.libraries, require, character = TRUE)
```

# Problem 1: Clustering and PCA
## Overview

In this exercise we are interested in applying and comparing unsupervised learning methods like K-means clustering and Principal Component Analysis to see whether they can successfully distinguish the red wines from the white wines, as well as sorting the higher from the lower quality of wines based on the dataset with 11 chemical properties of 6500 different bottles of vinho verde wine from northern Portugal. 

## Application of unsupervised learning methods to distinguish red wines from white wines

As a first step, the variables are explored to learn by which chemical properties the white and red wines differentiate most.

```{r Figure 1. boxplot_colors, echo = FALSE, warning=FALSE}
wine <- read_csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")

p1 <- qplot(x = color, y = fixed.acidity, data = wine, geom = "boxplot")
p2 <- qplot(x = color, y = volatile.acidity, data = wine, geom = "boxplot")
p3 <- qplot(x = color, y = citric.acid, data = wine, geom = "boxplot")
p4 <- qplot(x = color, y = residual.sugar, data = wine, geom = "boxplot")
p5 <- qplot(x = color, y = chlorides, data = wine, geom = "boxplot")
p6 <- qplot(x = color, y = free.sulfur.dioxide, data = wine, geom = "boxplot")
p7 <- qplot(x = color, y = total.sulfur.dioxide, data = wine, geom = "boxplot")
p8 <- qplot(x = color, y = density, data = wine, geom = "boxplot")
p9 <- qplot(x = color, y = pH, data = wine, geom = "boxplot")
p10 <- qplot(x = color, y = sulphates, data = wine, geom = "boxplot")
p11 <- qplot(x = color, y = alcohol, data = wine, geom = "boxplot")
p12 <- qplot(x = color, y = quality, data = wine, geom = "boxplot")

grid.arrange(p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, p12, nrow = 3, top = "Figure 1.1 Boxplots of chemical properties of white and red wines")
```

From these boxplots, it can be concluded that red and white wines differentiate mostly by chemical properties like total sulphur dioxide, residual sugar, volatile acidity and  density. These features will be used later to analyze the performance of the selected methods.

#### K-means clustering to distinguish colors of wine

K-means clustering was chosen as a first approach to differentiate the two types of wines by color. Accordingly, we set the number of clusters K in K-means as 2.Then we run K-means on 11 chemical properties(the first 11 columns of the dataset).   

The following two figures show visually that K-means clustering could successfully differentiate two colors of wines in relation to four chemical properties, by which red and white wines differentiate mostly. Additionally, in the first figure of boxplots we noticed that two colors of wines didn't differentiate much in the other chemical features. For this reason we chose to show visually the performance of K-means clustering on these four chemical features. For example, in the Figure 1.2. the top panel shows how k-means clustering partitioned the data into two clusters in the dimensions of fixed acidity and residual sugar. The bottom panel shows how actual wine colors differentiate in these dimensions. It can be inferred, that the two panels appear to be very similar visually. Figure 1.3. follows similar pattern.

```{r figure 1.2-1.3. K-means_colors, echo = FALSE, warning=FALSE}
# K-means clustering to distinguish colors of wine

# Center and scale the data
x_wine <- wine[,(1:11)]
x_wine <- scale(x_wine, center=TRUE, scale=TRUE)

# Run k-means with 2 clusters and 25 starts
set.seed(1)
clust1 <- kmeanspp(x_wine, 2, nstart=25) #automatically pick the best one

p1.1 <- qplot(fixed.acidity, residual.sugar , data=wine, color=factor(clust1$cluster))

p1.2 <- qplot(fixed.acidity, residual.sugar , data=wine, color=factor(wine$color))

grid.arrange(p1.1, p1.2, nrow = 2, top = "Figure 1.2. Results of K-means clustering for wine colors in the dimensions of fixed acidity and residual sugar")

p1.3 <- qplot(total.sulfur.dioxide, volatile.acidity, data=wine, color=factor(clust1$cluster))

p1.4 <- qplot(total.sulfur.dioxide, volatile.acidity, data=wine, color=factor(wine$color))

grid.arrange(p1.3, p1.4, nrow = 2, top = "Figure 1.3 Results of K-means clustering for wine colors in the dimensions of total sulfur dioxide and volatile acidity")
```

#### Prinicipal Components Analysis (PCA) to distinguish colors of wine

Next, Prinipal Components Analysis was performed in a similar attempt to distinguish the wines by two colors. The table below shows that the first five principal components explain cumulatively about 80% of the variation in the data. 

```{r , summary_pcw1, figure 1.4. PVE of pca, echo = FALSE, warning=FALSE}
#PCA
pca_wine <- prcomp(x_wine, scale = TRUE, center = TRUE)
summary_pca_wine <- data.frame(summary(pca_wine)$importance)
summary_pca_wine <- summary_pca_wine[ , 1:11]
kable(summary_pca_wine, caption="**Table 1.1. Summary table of PCA to distinguish colors**", format_caption = c("italic", "underline"), booktabs = TRUE) %>%
    kable_styling(bootstrap_options = "striped", full_width = T)

plot(pca_wine, main = "Figure 1.4. Proportions of variance explained by\n each principal component", xlab='Principal Components')
loadings <- pca_wine$rotation #red v vector
```

Below we can examine the loadings of the first 5 principal components. These are the linear combinations of the original chemical properties, which preserve as much of the information of those variables as possible. For example, free sulfur dioxide and total sulfur dioxide are relatively strongly and positively correlated with the first principal component, whereas volatile acidity and sulphates are relatively strongly and negatively correlated with the first principal component.

```{r , first five PCs, echo = FALSE, warning=FALSE}
rotation_pca_wine <- data.frame(round(pca_wine$rotation[, 1:5], 2))
kable(rotation_pca_wine, caption="**Table 1.2. Loadings of the first five principal components**", format_caption = c("italic", "underline")) %>%
    kable_styling(bootstrap_options = "striped", full_width = F)
```

Next, answering the question of where the individual data points fall in the space defined by first two principal components can help us identify how PCA performed the clustering of wines by their color. 

```{r , figure 1.5. pca for wine colors , echo = FALSE, warning=FALSE}
scores <- pca_wine$x
qplot(scores[, 1], scores[, 2], color = wine$color, xlab = 'Component 1', ylab = 'Component 2', main = "Figure 1.5. Results of PCA for wine colors in the dimensions of first two principal components")
```

As it can be seen from the plot above, PCA could also perform the clustering of two types of wine. Mainly, white wines tend to cluster to the positive direction in the dimension of first principal component, whereas red wines tend to cluster in the opposite direction.

Further, PCA results can be used to perform K-means clustering based on them. We performed the K-means clustering on the first five principal components to check whether we can enhance the partitioning of wine color done by K-means. Figure 1.6. illustrates that differentiation of wine colors was done very successfully by these augmented k-means clustering method.

```{r , k-means via pca colors, echo = FALSE, warning=FALSE}
clust1_pca <- kmeanspp(scores[,1:5], 2, nstart = 20)

p3.1 <- qplot(fixed.acidity, residual.sugar , data=wine, color=factor(clust1_pca$cluster))
p3.2 <- qplot(fixed.acidity, residual.sugar , data=wine, color=factor(wine$color))

grid.arrange(p3.1, p3.2, nrow = 2, top = "Figure 1.6 Results of K-means clustering via PCA for wine colors in the dimensions of fixed acidity and residual sugar")
```

Table 1.3. compares the results of two clustering of K=2 based on the total within-cluster sum of squares and the between-cluster sum of squares. Clustering via PCA has a relative improvement of about 25% in the total within-cluster sum of squares, and approximately the same between-cluster sum of squares.

```{r , table 1.3 SSE comparison, echo = FALSE, warning=FALSE}
totwithinbetweenss <- rbind("Total within-cluster sum of squares of Basic clustering " = clust1$tot.withinss, 
                           "Total within-cluster sum of squares of Clustering via PCA " = clust1_pca$tot.withinss, 
                           "Between-cluster sum of squares of Basic clustering " = clust1$betweenss, 
                           "Between-cluster sum of squares of Clustering via PCA " = clust1_pca$betweenss)
kable(totwithinbetweenss, caption="**Table 1.3 : Evaluating in-sample fit of two clusterings**")%>%
    kable_styling(full_width = FALSE)
```

To sum up, it can be said that K-means and PCA  could succesfully differentiate the white and red wines using only the “unsupervised information” contained in the data on chemical properties of wines. However, by reducing the dimensions of the features of the dataset by PCA and then applying K-means clustering could distinguish colors of wine at even better level, which could be seen from the reduced within-cluster sum of squares.

## Application of unsupervised learning methods to distinguish quality levels of wines

We have divided the wines into three main levels of qualities. Wines that scored higher that 7 were defined as high quality wines, lower than 5 - as low quality and from 5 to 7 as medium quality wines. 

```{r , Figure 1.7. Boxplots of chemical prop qualities, echo = FALSE, warning=FALSE}
wine$qualityindicator <- 
    ifelse(wine$quality <= 4, 'low', 
           ifelse(wine$quality <= 7, 'medium', 'high'))

wine$qualityindicator <- factor(wine$qualityindicator , levels=c("low", "medium", "high"))

pq1 <- qplot(x = qualityindicator, y = fixed.acidity, data = wine, geom = "boxplot")
pq2 <- qplot(x = qualityindicator, y = volatile.acidity, data = wine, geom = "boxplot")
pq3 <- qplot(x = qualityindicator, y = citric.acid, data = wine, geom = "boxplot")
pq4 <- qplot(x = qualityindicator, y = residual.sugar, data = wine, geom = "boxplot")
pq5 <- qplot(x = qualityindicator, y = chlorides, data = wine, geom = "boxplot")
pq6 <- qplot(x = qualityindicator, y = free.sulfur.dioxide, data = wine, geom = "boxplot")
pq7 <- qplot(x = qualityindicator, y = total.sulfur.dioxide, data = wine, geom = "boxplot")
pq8 <- qplot(x = qualityindicator, y = density, data = wine, geom = "boxplot")
pq9 <- qplot(x = qualityindicator, y = pH, data = wine, geom = "boxplot")
pq10 <- qplot(x = qualityindicator, y = sulphates, data = wine, geom = "boxplot")
pq11 <- qplot(x = qualityindicator, y = alcohol, data = wine, geom = "boxplot")

grid.arrange(pq1, pq2, pq3, pq4, pq5, pq6, pq7, pq8, pq9, pq10, pq11, nrow = 3, top = "Figure 1.7. Boxplots of chemical properties of three wine quality levels")
```

From these boxplots, it can be concluded that higher and lower quality wines differentiate mostly by chemical properties like volatile acidity, alcohol, free sulfur dioxide and chlorides. These features will be used later to analyze the performance of the selected methods.

#### K-means clustering to distinguish between higher and lower quality levels of wines

K-means clustering was performed to test whether it can also  differentiate the types of wines by quality level. As we have three main levels of qualities, accordingly the number of clusters K in K-means is 3.  

The following set of figures show visually results of K-means clustering in differentiating the qualities of wines in relation to four chemical properties. For example, in the Figure 1.8. the top left plot shows how k-means clustering partitioned the data into three clusters in the dimensions of volatile acidity and alcohol. The middle left plot shows how actual three wine quality levels differentiate in these dimensions. It is difficult to see, how lower and higher quality wines differ, because of the huge amount of middle quality wines that overlap with them, so the bottom left plot shows only the high and low qualities of wines in these dimensions. Although, the distribution pattern of low and high qualities appear to be similar with red and green clusters in the top plot, it cannot be said that they follow exact pattern as was in the case of partitioning the colors of wine. The right panel shows this comparison in the dimensions of other two chemical features.

```{r , Figure 1.8 K-means for qualities, echo = FALSE, warning=FALSE}
set.seed(2)

clust_quality <- kmeanspp(x_wine, 3, nstart=25)

pq1.1 <- qplot(alcohol, volatile.acidity,  data=wine, color=factor(clust_quality$cluster), alpha=I(0.3))
pq1.2 <- ggplot(wine, aes(x=alcohol, y = volatile.acidity, color=qualityindicator, alpha = I(0.3)))+
    geom_point(data = subset(wine, qualityindicator %in% c("low","high", "medium")))
pq1.8 <- ggplot(wine, aes(x=alcohol, y = volatile.acidity, color=qualityindicator, alpha = I(0.3)))+
    geom_point(data = subset(wine, qualityindicator %in% c("low","high")))
pq1.5 <- ggplot(wine, aes(free.sulfur.dioxide, chlorides))+
    geom_point(aes(color=factor(clust_quality$cluster),alpha = I(0.3)))
pq1.6 <- ggplot(wine, aes(x=free.sulfur.dioxide, y = chlorides, color=qualityindicator, alpha = I(0.3)))+
    geom_point(data = subset(wine, qualityindicator %in% c("low","high","medium")))
pq1.10 <- ggplot(wine, aes(x=free.sulfur.dioxide, y = chlorides, color=qualityindicator, alpha = I(0.3)))+
    geom_point(data = subset(wine, qualityindicator %in% c("low","high")))
grid.arrange(pq1.1, pq1.5, pq1.2, pq1.6, pq1.8, pq1.10, nrow = 3, top = "Figure 1.8. Results of K-means clustering for wine qualities.")
```

#### Prinicipal Components Analysis (PCA) to distinguish between higher and lower quality levels of wines

Next, Prinipal Components Analysis was performed in a similar attempt to distinguish the quality levels of wines. From the previous PCA on wine colors it is known that the first five principal components explain cumulatively about 80% of the variation in the data. As before,by answering the question of where the individual data points fall in the space defined by first two principal components we can identify how PCA performed the clustering of wines by their quality levels.

```{r , figure 1.9. pca qualities, echo = FALSE, warning=FALSE}
pc_wine_qualities <- prcomp(x_wine, scale = TRUE)
scores2 <- pc_wine_qualities$x
qplot(scores2[, 1], scores2[, 2], color = wine$qualityindicator, xlab = 'Component 1', ylab = 'Component 2', main = "Figure 1.9. Results of PCA for wine qualities in the dimensions of first two principal components", alpha = I(0.1))
```

As it can be seen from the plot above, now PCA couldn't perform the clustering of higher qualities of wine from the lower types in a very distinctive way. Mainly, higher types of wines tend to cluster to the right spectrum of the dimension of first principal component, whereas lower quality wines tend to cluster in the left.

With the same procedure as before, PCA results were used to perform K-means clustering based on them. We performed the K-means clustering on the first five principal components to check whether partitioning of wine quality types done by K-means could be enhanced. Figure 1.10 illustrates that differentiation of wines by quality types was done at a better level by these augmented k-means clustering method. For example, the top left plot shows how k-means clustering partitioned the data into three clusters in the dimensions of volatile acidity and alcohol. The middle left plot shows how actual three wine quality levels differentiate in these dimensions. It is difficult to see, how lower and higher quality wines differ, because of the huge amount of middle quality wines that overlap with them, so the bottom left plot shows only the high and low qualities of wines in these dimensions. Although the distribution pattern of low and high qualities appears to be similar with red and green clusters in the top plot, it cannot be said that they follow exact same pattern. The right panel shows this comparison in the dimensions of other two chemical features like chlorides and free sulfur dioxide.

```{r , figure 1.10. k-means via pca qualities, echo = FALSE,warning=FALSE}
#k-means with pca
set.seed(3)
pc <- kmeanspp(scores2[,1:5], 3, nstart=20)
pq1.11 <- qplot(alcohol, volatile.acidity,  data=wine, color=factor(pc$cluster), alpha=I(0.3))
pq1.12 <- ggplot(wine, aes(x=alcohol, y = volatile.acidity, color=qualityindicator, alpha = I(0.3)))+
    geom_point(data = subset(wine, qualityindicator %in% c("low","high", "medium")))
pq1.18 <- ggplot(wine, aes(x=alcohol, y = volatile.acidity, color=qualityindicator, alpha = I(0.3)))+
    geom_point(data = subset(wine, qualityindicator %in% c("low","high")))
pq1.15 <- ggplot(wine, aes(free.sulfur.dioxide, chlorides))+
    geom_point(aes(color=factor(pc$cluster),alpha = I(0.3)))
pq1.16 <- ggplot(wine, aes(x=free.sulfur.dioxide, y = chlorides, color=qualityindicator, alpha = I(0.3)))+
    geom_point(data = subset(wine, qualityindicator %in% c("low","high","medium")))
pq1.110 <- ggplot(wine, aes(x=free.sulfur.dioxide, y = chlorides, color=qualityindicator, alpha = I(0.3)))+
    geom_point(data = subset(wine, qualityindicator %in% c("low","high")))
grid.arrange(pq1.11, pq1.15, pq1.12, pq1.16, pq1.18, pq1.110, nrow = 3, top = "Figure 1.10. Results of K-means clustering via PCA for wine qualities.")
```

Table 1.4. compares the results of two clustering with K=3 based on the total within-cluster sum of squares and the between-cluster sum of squares. Clustering via PCA has a relative improvement of about 31% in the total within-cluster sum of squares, and approximately the same between-cluster sum of squares.

```{r , table 1.4 SSE comparison qualities, echo = FALSE, warning=FALSE}
totwithinbetweenss <- rbind("Total within-cluster sum of squares of Basic clustering " = clust_quality$tot.withinss, 
                           "Total within-cluster sum of squares of Clustering via PCA " = pc$tot.withinss, 
                           "Between-cluster sum of squares of Basic clustering " = clust_quality$betweenss, 
                           "Between-cluster sum of squares of Clustering via PCA " = pc$betweenss)

kable(totwithinbetweenss, caption="**Table 1.4 : Evaluating in-sample fit of two clusterings**")%>%
    kable_styling(full_width = FALSE)
```

To sum up, it can be said that neither K-means clustering nor PCA could very successfully differentiate the higher from the lower quality wines using only the "unsupervised information" contained in the data on chemical properties of wines. However, by reducing the dimensions of the features of the data set by PCA and then applying K-means clustering could distinguish quality levels of wine at better level, which could be seen from the reduced within-cluster sum of squares.

## Conclusion

In conclusion, we could say that K-means clustering applied together with PCA as a dimensionality reduction technique for the chemical properties of wines works better to differentiate the red and white colors of wine, compared to the same technique to distinguish higher from the lower quality wines.

# Problem 2: Market Segmentation
## Overview
Based on data of the its twitter account followers, the brand “NutrientH20” tries to find out possible approaches to segment those followers. The key for our market segmentation purpose is to find some identifiers to identify different clusters of consumers so that NutrientH20 can target its consumers more accurately in marketing or advertising. In analysis, we used three approaches to get there: K-means, Principal Component Analysis (PCA) with Hierarchical Clustering, and Hierarchical Clustering with K-means. The first two aimed at doing general market segmentation, and the third aimed at giving the previous clusters more accurate labels. 

## The Data
The data contains 7882 twitter followers (consumers) and 36 categories of their twitter posts (tweets). For each follower, the data gives the amount of tweets in each category, during seven days in June 2014. First, we deleted the observations with either nonzero “spam” tweets or nonzero “adult” tweets since most of those two kinds of tweets are likely posted by robots that are meaningless for marketing analysis. Then, we deleted “spam “and “adult”. At last, we transformed the number of tweets in each category into the frequency of tweets belonging to each category to better represent the preferences of consumers.

## K-means
### The Proper K
K-means is a method to partition observations or features with a high dimension P into lower K dimensions (P>=K). For our purpose, it is to partition consumers into K clusters. We used CH index to choose a proper value of K.

```{r setup 2.1, results='hide', warning=FALSE, echo=FALSE}
social_marketing <- read_csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv")

social_marketing <- filter(social_marketing, social_marketing$spam == 0 & social_marketing$adult == 0)

x <- social_marketing[ , c(-1, -36, -37)]
x <- x/rowSums(x)
x_scale <- scale(x, center = TRUE, scale = TRUE)

n <- nrow(x_scale)
k_grid <- seq(2, 20, by = 1)
CH_grids <- foreach(k = k_grid, .combine='c') %do% {
    cluster_ks = kmeans(x_scale, k, nstart=25)
    W = cluster_ks$tot.withinss
    B = cluster_ks$betweenss
    CH = (B/W)*((n-k)/(k-1))
}

plot(k_grid, CH_grids, main="Figure 2.1 CH Index") #choose k=10
```

The Figure 2.1 is the elbow plot of CH index. It is not so straightforward to choose a vivid elbow from the graph. Generally, values of K ranged from 10 to 15 seemed to be good candidates. To make the clustering as parsimonious as possible, we chose K = 10.

### The Result of K-means
K-means gave us 10 stable clusters of consumers. Then, for each cluster, we calculated the sum of frequencies of each category over all consumers in that cluster. After ordering the categories by sums of frequencies, the first five categories with higher sums of frequencies were picked out to represent that cluster. Also, the number of consumers in each cluster was counted. The ultimate result is shown in Table 2.1.

```{r setup 2.2,  warning=FALSE, echo=FALSE}
set.seed(1)
clust_k <- kmeans(x_scale, 10, nstart = 25)

result <- data.frame(clust_k$cluster) #which cluster?

resultCombinedData <- cbind(x, result$clust_k.cluster)

table_kmeans <- data.frame("Cluster" = character(7))

for (i in 1:10) {
    Cluster = filter(resultCombinedData, result$clust_k.cluster == i)
    ClusterMeans = as.data.table(colMeans(Cluster), keep.rownames=TRUE)
    colnames(ClusterMeans) <- c("Categories", "Cluster Means")
    ClusterMeans = ClusterMeans[order(-`Cluster Means`)]
    ClusterMeans = ClusterMeans[1:6,1]
    c = data.frame(nrow(Cluster))
    ClusterMeans = rbind(ClusterMeans,c, use.names=FALSE)
    table_kmeans = cbind(table_kmeans, ClusterMeans)
}
table_kmeans <- table_kmeans[-1, -1]
table_kmeans <- transpose(table_kmeans)
table_kmeans <- table_kmeans[order(table_kmeans[, ncol(table_kmeans)], decreasing = TRUE),]

rownames(table_kmeans) <- c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5", "Cluster 6",
                         "Cluster 7", "Cluster 8", "Cluster 9", "Cluster 10")
colnames(table_kmeans)=c("Category 1", "Category 2", "Category 3", "Category 4", "Category 5",
                         "Consumer No.")
kable(table_kmeans, caption ="**Table 2.1: Ten Clusters of K-means**" ,format_caption = c("bold", "underline")) %>%
    kable_styling(bootstrap_options = "striped", full_width = F)
```

## The Principal Component Analysis (PCA) + Hierarchical Clustering
### Choose a Scaled Dataset and the Number of Principal Components
Next, we did segmentation by PCA. Since PCA is sensitive to the scales of data to some degree, before choosing a proper number of principal components, the first thing to do is to compare the effects of differently scaled data on PCA. We used three differently scaled datasets to do that. The first dataset was the one transformed into frequencies. The second one was gotten by doing zero-mean standardization on the frequency dataset. The third one was gotten from doing zero-mean standardization on the original dataset, the one before doing frequency transformation. PVE, the proportion of variance of each principal component, was calculated after doing PCA on each dataset. The Figure 4.1 shows how PVE of each dataset varies with each principal component, where the blue line corresponds to the first dataset, the red line corresponds to the second dataset, and the green one corresponds to the third dataset. Around the elbow part where the number of principal components is about eight to twelve, the blue line has the lowest position, which means the first several principal components of blue one summarize more variance in data. Hence, we chose the frequency dataset to do PCA.

```{r setup 2.3,  warning=FALSE, echo=FALSE}
pca1 <- prcomp(x)
pve1 <- 100*pca1$sdev^2/sum(pca1$sdev^2) #pve is proportion of variance

pca2 <- prcomp(x, scale=TRUE)
pve2 <- 100*pca2$sdev^2/sum(pca2$sdev^2)

x2 <- social_marketing[ , c(-1, -36, -37)]
pca3 <- prcomp(x2, scale=TRUE)
pve3 <- 100*pca3$sdev^2/sum(pca3$sdev^2)

PC <- c(1:34)
pvecombine <- cbind.data.frame(PC, pve1, pve2, pve3)

ggplot(data=pvecombine)+
    geom_line(aes(x=PC,y=pve1), col="blue")+geom_point(aes(x=PC, y=pve1), col="blue", shape=1, size=2)+
    geom_line(aes(x=PC,y=pve2), col="red")+geom_point(aes(x=PC, y=pve2), col="red", shape=2, size=2)+
    geom_line(aes(x=PC,y=pve3), col="green3")+geom_point(aes(x=PC, y=pve3), col="green3", shape=3, size=2)+
    labs(y="PVE", x="Principal Component")+
    theme_bw()+
    labs(title = "Figure 2.2: PVE of PCA Based on Datasets Scaled Differently")+
    theme(plot.title = element_text(hjust = 0.5))
```

Also based on the location of elbow in Figure 2.2, the first ten principal components were chosen as a proper summary of data. Table 2.2 also justifies our choosing: the first ten principal components include around 73 percent variation in data, which means the rest twenty-four principal components only represent 27 percent variation in data. So, we believe that the first ten are representative enough for the data.

```{r setup 2.4,  warning=FALSE, echo=FALSE}
sum_pca1 <- data.frame(summary(pca1)$importance)
sum_pca1 <- sum_pca1[ , 1:10]
kable(sum_pca1, caption ="**Table 2.2: Summary of Variations in Principal Components**", format_caption = c("italic", "underline")) %>%
    kable_styling(bootstrap_options = "striped", full_width = T)
```

### The Potential Problems of First Ten Principal Components
The first ten principal components can be used to differentiate consumers. In each principal component, the importance of categories was measured by loadings. Those categories assigned with high positive loadings were supposed to be closer to each other than other categories so that they were more likely to be in the same cluster.The result was presented in Table 2.3. 

```{r setup 2.5,  warning=FALSE, echo=FALSE}
pca1_loading <- pca1$rotation[ , c(1:10)]

pc <- data.frame(rep(1:34))
for (i in c(1:10)){
    sort = data.frame(sort(pca1_loading[,i], decreasing = TRUE))
    rownames = data.frame(row.names(sort))
    c = cbind(rownames, sort[,1])
    pc = cbind(pc,c)
}
pc <- pc[,-1]
colnames(pc) <- c("Categories", "PC1", "Categories", "PC2", "Categories", "PC3", "Categories", "PC4", "Categories", "PC5",
               "Categories", "PC6", "Categories", "PC7", "Categories", "PC8", "Categories", "PC9", "Categories", "PC10")

tablePCA <- pc[1:5,-c(seq(2, 20, by=2))]

colnames(tablePCA) <- c("PC 1", "PC 2", "PC 3", "PC 4", "PC 5", "PC 6",
                     "PC 7", "PC 8", "PC 9", "PC 10")
rownames(tablePCA) <- c("Category 1", "Category 2", "Category 3", "Category 4", "Category 5")

kable(tablePCA,  caption ="**Table 2.3: The Result of First 10 Principal Components**", format_caption = c("italic", "underline")) %>%
    kable_styling(bootstrap_options = "striped", full_width = T)
```

But there are two problems in the above result: it includes only 21 categories out of 34, and we do not know yet how many consumers have the bigest interest in each cluster and who they are, which makes us a few miles away from our destination.

### Use Hierarchical Clustering to Make a Better Clustering
To solve the problems, we used the scores (indices to indicate the positions of each observation in the projected space) of PCA to construct a distance matrix, which measures the distance between each pair of consumers in the data. By Hierarchical Clustering and cutting the resulting dendrogram, we got 10 clusters of consumers, in accordance with the value of K in K-means. Afterwards, we did the same thing as in K-means after getting 10 clusters. The result is shown in Table 2.4 (To be more distinguishable, we named the clusters as “Pcluster” here). It is the exactly same as in K-means, which strongly implies the robustness of partitioning the market into 10 clusters.

```{r setup 2.6,  warning=FALSE, echo=FALSE}
pca1 <- prcomp(x)
pca1_scores <- pca1$x[ , c(1:10)]

D0 <- dist(pca1_scores)
hclust10 <- hclust(D0, method='complete')
cluster_pca1_10 <- cutree(hclust10, 10)

resultp <- data.frame(cluster_pca1_10)

resultpCombinedData <- cbind(x, result$clust_k.cluster)

table_pca <- data.frame("Cluster"=character(7))
for (i in 1:10) {
    Cluster = filter(resultpCombinedData, result$clust_k.cluster == i)
    ClusterMeans = as.data.table(colMeans(Cluster), keep.rownames=TRUE)
    colnames(ClusterMeans) <- c("Categories", "Cluster Means")
    ClusterMeans = ClusterMeans[order(-`Cluster Means`)]
    ClusterMeans = ClusterMeans[1:6,1]
    c=data.frame(nrow(Cluster))
    ClusterMeans=rbind(ClusterMeans,c, use.names=FALSE)
    table_pca=cbind(table_pca, ClusterMeans)
}

table_pca <- table_pca[-1, -1]
table_pca <- transpose(table_pca)
table_pca <- table_pca[order(table_pca[, ncol(table_pca)], decreasing = TRUE),]
rownames(table_pca) <- c("PCluster 1", "PCluster 2", "PCluster 3", "PCluster 4", "PCluster 5", "PCluster 6",
                      "PCluster 7", "PCluster 8", "PCluster 9", "PCluster 10")
colnames(table_pca) <- c("Category 1", "Category 2", "Category 3", "Category 4", "Category 5",
                      "Consumer No.")
kable(table_pca,  caption ="**Table 2.4: An Improved Result of Hierarchical Clustering**", format_caption = c("italic", "underline")) %>%
    kable_styling(bootstrap_options = "striped", full_width = F)
```

## Can We Have More Accurate, Unique or Complete Labels?
The first problem of the result in Table 2.3 also exists in Table 2.2 and Table 2.4, though in a much less evident way: 27 categories out of 34 categories are used to differentiate consumers into 10 clusters. Probably it is because the rest 7 categories are not discernable or important enough based on the data, but it could be the possibility that our method was not refined enough. Of course, we could pick out more categories, say, first six or seven categories with high sums of frequencies, from each cluster, to include categories as many as possible. However, usually the categories below the third one only have relatively much smaller sums of frequencies than those of the first three categories. So, including more categories would be likely to put more infrequent or accident tweets into the description of distinct preferences. Moreover, some clusters may be not so distinguishable from other clusters. For example, Cluster 4 and 5 are more likely to consist of the same group of people. The borders between those clusters are not so clear. Therefore, in this part, for each of 10 clusters, we tried to find out the unique category or the combination of several unique categories dominating all other categories by the sum of frequencies. Meanwhile, all those unique category or combinations of unique categories made up a partition of 34 categories. By doing so, we were attaching labels to each cluster to make them more distinguishable.

The approach we applied was Hierarchical Clustering with K-means. The Hierarchical Clustering was to find out a hierarchical structure of categories based on the proximity matrix of correlations between each pair of categories. Figure 2.3 shows the dendrogram. 

```{r setup 2.7, results='hide', warning=FALSE, echo=FALSE}
xs_corr <- cor(x_scale)
d_xscorr <- dist(xs_corr)

hc_corr1 <- hclust(d_xscorr, method = "complete")
plot(hc_corr1, cex=0.8, xlab="", sub="", main="Figure 2.3: Dendrogram of Hierarchical Clustering of Categories")
```

Then, we horizontally cut the dendrogram into ten clusters of categories as a partition of 34 categories. Based on the partition, we added up the categories belonging to the same cluster in the frequency dataset, to get a new dataset with 10 clusters. Then we used K-means to split consumers into 10 clusters of consumers and applied the same procedure as in K-means to attach ten clusters of categories to ten clusters of consumers. The result is shown in Table 2.5 (Again, to be distinguishable, we named clusters as “Hcluster” here). 

```{r setup 2.8,  warning=FALSE, echo=FALSE}
hc_corr10 <- cutree(hc_corr1, 10)

hc_corr10_table <- data.frame(hc_corr10)
rownames <- data.frame(rownames(hc_corr10_table))
hc_corr10_table <- cbind(rownames, hc_corr10_table$hc_corr10)
colnames(hc_corr10_table) <- c("Category", "Hcluster")
hc_corr10_table <- hc_corr10_table[order(hc_corr10_table$Hcluster), ]
rownames(hc_corr10_table) <- c()


xclust10 <- x %>% transmute(c1=chatter+photo_sharing+shopping, c2=eco+current_events+uncategorized+home_and_garden+ music
                         +business+small_business, c3=travel+ politics+computers, c4=tv_film+ crafts +art, c5=sports_fandom
                         +food+family+religion+parenting+school, c6= news+automotive, c7= online_gaming+college_uni
                         +sports_playing, c8=health_nutrition+ outdoors+personal_fitness, c9=cooking+beauty +fashion,
                         c10=dating)
## kmeans
xclust_scale10 <- scale(xclust10, center = TRUE, scale = TRUE)
kmeans10 <- kmeans(xclust_scale10, 10, nstart = 25)
cluster_k10 <- data.frame(kmeans10$cluster)
xclustcombine10 <- cbind(xclust10, cluster_k10)
colnames(xclustcombine10)[11]="cluster"

result_save10 <- data.frame("Cluster"=character(10), "Consumers No."=numeric(10), "cluster in kmeans"=numeric(10),stringsAsFactors=FALSE)
for (i in 1:10){
    group=filter(xclustcombine10, cluster==i)[, -11]
    gmean=data.frame(apply(group, 2, mean))
    result_save10[i,]=c(rownames(gmean)[apply(gmean, 2, which.max)], nrow(group), i)
}
result_save10 <- result_save10[order(result_save10$cluster), ]
rownames(result_save10) <- c()
result_save10[which(result_save10$Consumers.No.==297) ,1]=colnames(xclust_scale10)[10]

tableclust <- data.frame()
for (j in c("c1", "c2", "c3", "c4", "c5", "c6", "c7", "c8", "c9", "c10")){
    a=result_save10[which(result_save10$Cluster==j), ]
    tableclust=rbind(tableclust,a)
}

Core_Categories <- c("chatter, photo_sharing, shopping", "eco, current_events, uncategorized, home_and_garden, music, 
             business, small_business", "travel, politics, computers", "tv_film, crafts, art", "sports_fandom, food, family, religion, parenting, school", "news, automotive", "online_gaming, college_uni, sports_playing", "health_nutrition, outdoors, personal_fitness", "cooking, beauty, fashion", "dating" )

tableclust <- cbind(tableclust, Core_Categories)[ ,-3]
tableclust <- tableclust[order(tableclust[, 2], decreasing = TRUE),]

rownames(tableclust) <- c("Hcluster 1", "Hcluster 2", "Hcluster 3", "Hcluster 4", "Hcluster 5", "Hcluster 6",
                       "Hcluster 7", "Hcluster 8", "Hcluster 9", "Hcluster 10")
tableclust <-tableclust[,-1]

kable(tableclust, caption ="**Table 2.5: Result of Hierarachical Clustering with K-means**", format_caption = c("italic", "underline")) %>%
    kable_styling(bootstrap_options = "striped", full_width = F)
```

One good news is that there exists one to one mapping between ten clusters of categories and ten clusters of consumers. The other good news is that the distribution of the numbers of consumers over ten clusters is extremely close to the distribution in Table 2.3 or Table 2.4. Hence, by comparing the number of consumers, it is straightforward to link the clusters in Table 2.5 to those clusters in Table 2.3 or Table 2.4. The stable clustering structures across different approaches proves that our ten clusters are very robust based on our data.

It should be noted that the labels in Table 2.5 are supplements to categories of each cluster in Table 2.3 or Table 2.4. By labels in Table 2.5, the brand can try to do more accurate advertising or marketing for consumers in any clusters. It also helps the brand to recognize a new consumer. For example, if a new consumer has some crafts posts, just by categories in Table 2.3 or 2.4, it is difficulty to put her into any cluster. But by labels in Table 2.5, we know crafts is more likely to be clustered with movie and art, so it is reasonable to put her in Cluster 6 before having more information about this new consumer.

## Conclusion
Based on the above ten clusters of consumers, now we can make some comments.

The largest cluster of the audience of NurientH20 was those who focused much on eco or home and garden. The second largest paid more attention to cooking, beauty, and fashion. The third to fifth largest clusters focused more on news, travel, politics. All those largest five clusters probably could be grouped into a more general consumer type: less likely to be students, perhaps having families and children, female teenagers who have interests in beauty and fashion, or businessmen. Also, they were more likely to have high incomes. Therefore, this type of consumers could be in priority in the marketing of NutrientH20. 

The second type of consumers might be younger people. Most of them probably were college students with those classical student preferences like online gaming, sporting, watching movies and TV shows, dating and so forth. This type of consumers might have minor purchase power, but they could be potential future consumers of the brand, with more permanent brand loyalty. Hence, it was also important for NutrientH20 to attract those type of consumers as many as possible. 

# Problem 3: Groceries

```{r setup 2.9, results='hide', warning=FALSE, echo=FALSE}
groceries <- read_lines("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/groceries.txt")

#clean data
clean_groceries <- list()
for (i in 1:length(groceries)) {
    clean_groceries[i] <- str_split(groceries[i], ",") %>% unique()
}

#cast to transactions class
clean_groceries_trans <- as(clean_groceries, "transactions")
summary(clean_groceries_trans)
```

```{r setup 2.10, warning=FALSE, echo=FALSE}
# number of items in each observation
size(head(clean_groceries_trans)) 

#average number of items purchased
mean(
    matrix(data= c(lengths(clean_groceries)),
       ncol = 1,
       byrow = TRUE)
)
```

Because the average number of items purchased is approximately 4, it seems appropriate to set `maxlen` equals to 4. I also set `support` which is marginal probability to be 1% and `confidence` equals to 50%. It is worth reminding that the higher value of `confidence` parameter, the stronger the rules. Also, low support does not preclude high confidence or high lift. Once you know that something is purchased (although rarely purchased), you know with high probability that another item is likely to be bought too.

```{r setup 2.11, warning=FALSE, echo=FALSE}
#now run the 'apriori' algorithm
#maxlen defines the maximum number of items in each itemset of frequent items
groceries_rules <- apriori(clean_groceries_trans,
                 parameter = list(support = 0.01, 
                                  confidence = 0.5,
                                  maxlen = 4))
# Look at the output... so many rules!                 
arules::inspect(groceries_rules)

## Choose a subset
arules::inspect(subset(groceries_rules, lift > 2))
```

## How to get the product recommendation rules?
### High-confidence rules

Event with high confidence means the probability of RHS(consequent) given LHS(antecedent) is high. The rule with confidence of 1 imply that when LHS item was purchased, the RHS item was also purchased 100% of the time.

```{r setup 2.12, warning=FALSE, echo=FALSE}
#'high-confidence' rules.
high_conf <- sort(groceries_rules, by = "confidence", decreasing = TRUE)

# show the support, lift and confidence for all rules
arules::inspect(head(high_conf))
```

### High-lift rules

Lift is calculated by taking confidence divided by support. Generally, association rules with high lift are most useful because they tell you something you don't already know. Rules with high lift mean that items in LHS and RHS are more likely to be bought together compared to the purchases when they are assumed to be unrelated.

```{r setup 2.13, warning=FALSE, echo=FALSE}
# 'high-lift' rules.
high_lift <- sort(groceries_rules, by = "lift", decreasing=TRUE) 

# show the support, lift and confidence for all rules
arules::inspect(head(high_lift)) 
```

## Most frequenly purchased items

We see that whole milk is the most commonly purchased item followed by vegetables and buns. This makes sense because they are all basic foods.

```{r setup 2.14, results='hide', warning=FALSE, echo=FALSE}
#most frequent items
frequent_items <- eclat(clean_groceries_trans, parameter = list(supp = 0.07, maxlen = 15)) # calculates support for frequent items

arules::inspect(frequent_items)
```

```{r setup 2.15, warning=FALSE, echo=FALSE}
itemFrequencyPlot(clean_groceries_trans, topN = 10, type = "absolute", main = "Item Frequency")
```

## Customers who bought ‘Whole Milk’ also bought
Let's find out what customers had purchased before buying ‘Whole Milk’. This will help us understand the patterns that led to the purchase of ‘whole milk’.

```{r setup 2.16, warning=FALSE, echo=FALSE}
#Customers who bought ‘Whole Milk’ also bought
milk <- apriori(data = clean_groceries_trans, parameter = list (supp=0.001, conf = 0.15, minlen = 2), appearance = list(default = "rhs",lhs = "whole milk"), control = list (verbose=F))

# 'high-confidence' rules.
milk_conf <- sort(milk, by = "confidence", decreasing=TRUE) 
arules::inspect(head(milk_conf))
```

In the following graph, we observe negative relationship between support and lift. When you have high support, it means that the items is commonly purchased. It's like almost everyone buys it. So it is not very helpful or telling you much information.

```{r setup 2.17, warning=FALSE, echo=FALSE}
# plot all the rules in (support, confidence) space
plot(groceries_rules)

# can swap the axes and color scales
plot(groceries_rules, measure = c("support", "lift"), shading = "confidence")
```

In the graph below, we see that the highest confidence tends to be order 3 (order 3 has more items). The more items you condition on, the more sure.

```{r setup 2.18, warning=FALSE, echo=FALSE}
# "two key" plot: coloring is by size (order) of item set
plot(groceries_rules, method='two-key plot')
```

```{r setup 2.19, warning=FALSE, echo=FALSE}
# can now look at subsets driven by the plot
arules::inspect(subset(groceries_rules, support > 0.04))
arules::inspect(subset(groceries_rules, confidence > 0.4))

# export a graph
sub1 = subset(groceries_rules, subset=confidence > 0.01 & support > 0.005)
saveAsGraph(sub1, file = "groceries_rules.graphml")
#png::readPNG(source = "hw4graph.png")

img <- readPNG('hw4.png')
grid::grid.raster(img)
```

# Graph-Based Visualization: Top 30 rules by lift

```{r setup 2.20, warning=FALSE, echo=FALSE}
plot(head(sub1, 30, by='lift'), method='graph')
```